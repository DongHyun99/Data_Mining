{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python383jvsc74a57bd0579282893146967bc5d17141c601e8a8b8ad4a0a2a5a6fe1c87b1000077400d7",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "This dataset consists of 101 animals from a zoo.  \n",
    "There are 16 variables with various traits to describe the animals.  \n",
    "The 7 Class Types are: Mammal, Bird, Reptile, Fish, Amphibian, Bug and Invertebrate  \n",
    "\n",
    "The purpose for this dataset is to be able to predict the classification of the animals, based upon the variables.  \n",
    "It is the perfect dataset for those who are new to learning Machine Learning.  \n",
    "\n",
    "# zoo.csv  \n",
    "Attribute Information: (name of attribute and type of value domain)  \n",
    "\n",
    "1. animal_name: Unique for each instance  \n",
    "2. hair Boolean  \n",
    "3. feathers Boolean  \n",
    "4. eggs Boolean  \n",
    "5. milk Boolean  \n",
    "6. airborne Boolean  \n",
    "7. aquatic Boolean  \n",
    "8. predator Boolean  \n",
    "9. toothed Boolean  \n",
    "10. backbone Boolean  \n",
    "11. breathes Boolean  \n",
    "12. venomous Boolean  \n",
    "13. fins Boolean  \n",
    "14. legs Numeric (set of values: {0,2,4,5,6,8})  \n",
    "15. tail Boolean  \n",
    "16. domestic Boolean  \n",
    "17. catsize Boolean  \n",
    "18. class_type Numeric (integer values in range [1,7])  \n",
    "\n",
    "# class.csv  \n",
    "This csv describes the dataset  \n",
    "\n",
    "1. Class_Number Numeric (integer values in range [1,7])  \n",
    "2. NumberOfAnimalSpeciesIn_Class Numeric  \n",
    "3. Class_Type character -- The actual word description of the class  \n",
    "4. Animal_Names character -- The list of the animals that fall in the category of the class  \n",
    "\n",
    "# Acknowledgements  \n",
    "UCI Machine Learning: https://archive.ics.uci.edu/ml/datasets/Zoo  \n",
    "\n",
    "Source Information\n",
    "-- Creator: Richard Forsyth  \n",
    "-- Donor: Richard S. Forsyth  \n",
    "8 Grosvenor Avenue  \n",
    "Mapperley Park  \n",
    "Nottingham NG3 5DX  \n",
    "0602-621676  \n",
    "-- Date: 5/15/1990  \n",
    "\n",
    "# Inspiration  \n",
    "What are the best machine learning ensembles/methods for classifying these animals based upon the variables given?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "101마리의 동물원 동물에 대한 특징 16가지를 수집한 DataSet이다.  \n",
    "zoo.csv는 각 동물과 특성에 대한 값들을 표기했다.  \n",
    "class.csv는 동물을 laveling한 값들을 표기했다.   \n",
    "Dummy Data Encoder를 사용해 보기위해 데이터를 가져와 봤는데 다리의 개수를 제외한 값들이 이미 처리가 되어있어서 그냥 데이터 분석을 해봐야 할것같다.\n",
    "\n",
    "# DataSet 받아오기"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join('data/zoo') # Data save folder\n",
    "\n",
    "def load_zoo_data(): # Loading Data\n",
    "    csv_path = os.path.join(DATA_PATH,'zoo.csv')\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "def load_label_data(): # Loading Data\n",
    "    csv_path = os.path.join(DATA_PATH,'class.csv')\n",
    "    return pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 101 entries, 0 to 100\nData columns (total 18 columns):\n #   Column       Non-Null Count  Dtype \n---  ------       --------------  ----- \n 0   animal_name  101 non-null    object\n 1   hair         101 non-null    int64 \n 2   feathers     101 non-null    int64 \n 3   eggs         101 non-null    int64 \n 4   milk         101 non-null    int64 \n 5   airborne     101 non-null    int64 \n 6   aquatic      101 non-null    int64 \n 7   predator     101 non-null    int64 \n 8   toothed      101 non-null    int64 \n 9   backbone     101 non-null    int64 \n 10  breathes     101 non-null    int64 \n 11  venomous     101 non-null    int64 \n 12  fins         101 non-null    int64 \n 13  legs         101 non-null    int64 \n 14  tail         101 non-null    int64 \n 15  domestic     101 non-null    int64 \n 16  catsize      101 non-null    int64 \n 17  class_type   101 non-null    int64 \ndtypes: int64(17), object(1)\nmemory usage: 14.3+ KB\n"
     ]
    }
   ],
   "source": [
    "zoo = load_zoo_data()\n",
    "label = load_label_data()\n",
    "zoo.info()"
   ]
  },
  {
   "source": [
    "# Label Data 정리"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_label=[] # 동물을 label에 따라 묶어놓음\n",
    "labelling=[] # 각 동물이 속해져 있는 label\n",
    "\n",
    "for i in range(0,7,1):\n",
    "    new_label.append(label.Animal_Names[i].split(\", \"))\n",
    "\n",
    "for i in zoo.animal_name:\n",
    "    for j in range(0,7,1):\n",
    "        if i in new_label[j]:\n",
    "            labelling.append(j)"
   ]
  },
  {
   "source": [
    "# pre-processing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "# +로그 변환환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoo_val = zoo.iloc[:,1:]\n",
    "\n",
    "mm = MinMaxScaler()\n",
    "ma = MaxAbsScaler()\n",
    "ss = StandardScaler()\n",
    "rs = RobustScaler()\n",
    "no = Normalizer()\n",
    "\n",
    "zoo_1 = pd.DataFrame(mm.fit_transform(zoo_val), columns=zoo_val.columns)\n",
    "zoo_2 = pd.DataFrame(ma.fit_transform(zoo_val), columns=zoo_val.columns)\n",
    "zoo_3 = pd.DataFrame(ss.fit_transform(zoo_val), columns=zoo_val.columns)\n",
    "zoo_4 = pd.DataFrame(rs.fit_transform(zoo_val), columns=zoo_val.columns)\n",
    "zoo_5 = pd.DataFrame(no.fit_transform(zoo_val), columns=zoo_val.columns)\n",
    "zoo_6 = pd.DataFrame(np.log1p(zoo_val))"
   ]
  },
  {
   "source": [
    "# K-Means"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "not_Scaled:  0.0\nMinMax_Scaled:  0.039603960396039604\nMaxAbs_Scaled:  0.13861386138613863\nStrandard_Scaled:  0.039603960396039604\nRobust_Scaled:  0.5247524752475248\nNormalizer_Scaled:  0.40594059405940597\nLog_Scaled:  0.6039603960396039\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "km = KMeans(n_clusters=4, random_state=1)\n",
    "predict0 = pd.DataFrame(km.fit_predict(zoo_val), columns=['predict'])\n",
    "predict1 = pd.DataFrame(km.fit_predict(zoo_1), columns=['predict'])\n",
    "predict2 = pd.DataFrame(km.fit_predict(zoo_2), columns=['predict'])\n",
    "predict3 = pd.DataFrame(km.fit_predict(zoo_3), columns=['predict'])\n",
    "predict4 = pd.DataFrame(km.fit_predict(zoo_4), columns=['predict'])\n",
    "predict5 = pd.DataFrame(km.fit_predict(zoo_5), columns=['predict'])\n",
    "predict6 = pd.DataFrame(km.fit_predict(zoo_6), columns=['predict'])\n",
    "\n",
    "# 공부해보니 Accuracy score이라는 train data의 클러스터링 결과를 평가하는 함수가 있어 사용해 보았다. (올바른 데이터의 수/전체데이터의 수)\n",
    "\n",
    "print('not_Scaled: ', accuracy_score(labelling,predict0))\n",
    "print('MinMax_Scaled: ', accuracy_score(labelling,predict1))\n",
    "print('MaxAbs_Scaled: ', accuracy_score(labelling,predict2))\n",
    "print('Strandard_Scaled: ', accuracy_score(labelling,predict3))\n",
    "print('Robust_Scaled: ', accuracy_score(labelling,predict4))\n",
    "print('Normalizer_Scaled: ', accuracy_score(labelling,predict5))\n",
    "print('Log_Scaled: ', accuracy_score(labelling,predict6))"
   ]
  },
  {
   "source": [
    "K-means 알고리즘으로 클러스터링 했을 때 log_scale 했을 때 가장 높은 정확도인 60%가량의 결과를 보인다.\n",
    "\n",
    "# Dicision Tree Classifier\n",
    "\n",
    "저번시간에 봤던 Decision Tree로 yes/no를 정해서 구분하는 것이 동물의 과를 정하기 제격이라는 생각이 들어 시도해 봤다.  \n",
    "Decision Tree의 장점 중 하나가 scailer의 영향을 받지 않는다는 것인데 Normalizer를 제외하고 결과 값에 영향이 실제로 있는지 직접 체크해 봐야겠다.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}