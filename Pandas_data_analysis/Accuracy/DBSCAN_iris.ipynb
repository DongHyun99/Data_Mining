{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "ee6caf572b02fbd90c9d9a82b73a7abd542127d236c55761b702f851ee4a295f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 2. DBSCAN으로 학습하기 (iris)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.datasets import load_wine\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data load\n",
    "\n",
    "iris = load_iris()\n",
    "wine = load_wine()\n",
    "\n",
    "iris.data = pd.DataFrame(iris.data)\n",
    "iris.target = pd.DataFrame(iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\neps의 값은 항상 positive 해야 하므로 for문에 넣을 수 없다.\\n따라서 계획 했던대로 eps의 값을 증가 시켜보며 학습하는 것은 힘들고\\nminpts의 값을 증가시키면서 학습해보겠다.\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 111
    }
   ],
   "source": [
    "# Specify the eps value\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "'''\n",
    "eps의 값은 항상 positive 해야 하므로 for문에 넣을 수 없다.\n",
    "따라서 계획 했던대로 eps의 값을 증가 시켜보며 학습하는 것은 힘들고\n",
    "minpts의 값을 증가시키면서 학습해보겠다.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "    0\n5   3\n6   3\n7   3\n8   3\n9   3\n10  3\n14  3\n15  3\n16  3\n"
     ]
    }
   ],
   "source": [
    "# class count graph with minpts change\n",
    "# min_samples는 1~100까지 돌려본다.\n",
    "\n",
    "class_count=[0,]\n",
    "\n",
    "for i in range(1,100,1):\n",
    "    dbscan = DBSCAN(min_samples=i)\n",
    "    result = dbscan.fit_predict(iris.data)\n",
    "    class_count.append(len(np.unique(result)))\n",
    "\n",
    "class_count = pd.DataFrame(class_count)\n",
    "class_count = class_count[class_count[0]==3]\n",
    "print(class_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "minpts= 5 : 0.6044156375501387\nminpts= 6 : 0.6010609730489903\nminpts= 7 : 0.5825248082208494\nminpts= 8 : 0.5842137354876208\nminpts= 9 : 0.5871440773125185\nminpts= 10 : 0.5912526142204563\nminpts= 14 : 0.6338804521368303\nminpts= 15 : 0.6145305039043192\nminpts= 16 : 0.6083339269265271\n"
     ]
    }
   ],
   "source": [
    "# normal_iris_dbscan accuracy\n",
    "# class의 개수가 데이터의 개수와 맞는 것의 accuracy를 측정해본다.\n",
    "\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "\n",
    "for i in class_count.index:\n",
    "    dbscan = DBSCAN(min_samples=i)\n",
    "    result = dbscan.fit_predict(iris.data)\n",
    "    print('minpts=',i,\":\",normalized_mutual_info_score(iris.target[0], result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaler Preprocessed DBSCAN Accuracy\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "data_minmax = MinMaxScaler().fit_transform(iris.data)\n",
    "data_maxabs = MaxAbsScaler().fit_transform(iris.data)\n",
    "data_standard = StandardScaler().fit_transform(iris.data)\n",
    "data_robust = RobustScaler().fit_transform(iris.data)\n",
    "data_normalize = Normalizer().fit_transform(iris.data)\n",
    "data_log = np.log1p(iris.data)\n",
    "\n",
    "class_count_minmax, class_count_maxabs, class_count_standard, class_count_robust, class_count_normalize, class_count_log = [],[],[],[],[],[]\n",
    "\n",
    "for i in range(1,100,1):\n",
    "    dbscan = DBSCAN(min_samples=i)\n",
    "    result_minmax = dbscan.fit_predict(data_minmax)\n",
    "    result_maxabs = dbscan.fit_predict(data_maxabs)\n",
    "    result_standard = dbscan.fit_predict(data_standard)\n",
    "    result_robust = dbscan.fit_predict(data_robust)\n",
    "    result_normalize = dbscan.fit_predict(data_normalize)\n",
    "    result_log = dbscan.fit_predict(data_log)\n",
    "\n",
    "    class_count_minmax.append(len(np.unique(result_minmax)))\n",
    "    class_count_maxabs.append(len(np.unique(result_maxabs)))\n",
    "    class_count_standard.append(len(np.unique(result_standard)))\n",
    "    class_count_robust.append(len(np.unique(result_robust)))\n",
    "    class_count_normalize.append(len(np.unique(result_normalize)))\n",
    "    class_count_log.append(len(np.unique(result_log)))\n",
    "\n",
    "class_count_minmax = pd.DataFrame(class_count_minmax)\n",
    "class_count_minmax = class_count_minmax[class_count_minmax[0]==3]\n",
    "class_count_maxabs = pd.DataFrame(class_count_maxabs)\n",
    "class_count_maxabs = class_count_maxabs[class_count_maxabs[0]==3]\n",
    "class_count_standard = pd.DataFrame(class_count_standard)\n",
    "class_count_standard = class_count_standard[class_count_standard[0]==3]\n",
    "class_count_robust = pd.DataFrame(class_count_robust)\n",
    "class_count_robust = class_count_robust[class_count_robust[0]==3]\n",
    "class_count_normalize = pd.DataFrame(class_count_normalize)\n",
    "class_count_normalize = class_count_normalize[class_count_normalize[0]==3]\n",
    "class_count_log = pd.DataFrame(class_count_log)\n",
    "class_count_log = class_count_log[class_count_log[0]==3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_minmax, score_maxabs, score_standard, score_robust, score_normalize, score_log = [], [], [], [], [], []\n",
    "\n",
    "def minpts_score(arr, data, score_list):\n",
    "    for i in arr.index:\n",
    "        dbscan = DBSCAN(min_samples=i)\n",
    "        result = dbscan.fit_predict(data)\n",
    "        score_list.append(normalized_mutual_info_score(iris.target[0], result))\n",
    "\n",
    "minpts_score(class_count_minmax, data_minmax, score_minmax)\n",
    "minpts_score(class_count_maxabs, data_maxabs, score_maxabs)\n",
    "minpts_score(class_count_standard, data_standard, score_standard)\n",
    "minpts_score(class_count_robust, data_robust, score_robust)\n",
    "minpts_score(class_count_normalize, data_normalize, score_normalize)\n",
    "minpts_score(class_count_log, data_log, score_log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "minmax score: 0\nmaxabs score: 0\nstandard score: 0.5168393648274324\nrobust score: 0.66438908517785\nnormalize score: 0\nlog score: 0\n"
     ]
    }
   ],
   "source": [
    "def not_value(string, value):\n",
    "    try:\n",
    "        print(string, max(value))\n",
    "    except ValueError:\n",
    "        print(string, 0)\n",
    "\n",
    "not_value('minmax score:', score_minmax)\n",
    "not_value('maxabs score:', score_maxabs)\n",
    "not_value('standard score:', score_standard)\n",
    "not_value('robust score:', score_robust)\n",
    "not_value('normalize score:', score_normalize)\n",
    "not_value('log score:', score_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "model = PCA(n_components=2)\n",
    "\n",
    "data_minmax = MinMaxScaler().fit_transform(iris.data)\n",
    "data_maxabs = MaxAbsScaler().fit_transform(iris.data)\n",
    "data_standard = StandardScaler().fit_transform(iris.data)\n",
    "data_robust = RobustScaler().fit_transform(iris.data)\n",
    "data_normalize = Normalizer().fit_transform(iris.data)\n",
    "data_log = np.log1p(iris.data)\n",
    "\n",
    "data_minmax = model.fit_transform(data_minmax)\n",
    "data_maxabs = model.fit_transform(data_maxabs)\n",
    "data_standard = model.fit_transform(data_standard)\n",
    "data_robust = model.fit_transform(data_robust)\n",
    "data_normalize = model.fit_transform(data_normalize)\n",
    "data_log = model.fit_transform(data_log)\n",
    "\n",
    "class_count_minmax, class_count_maxabs, class_count_standard, class_count_robust, class_count_normalize, class_count_log = [],[],[],[],[],[]\n",
    "\n",
    "for i in range(1,100,1):\n",
    "    dbscan = DBSCAN(min_samples=i)\n",
    "    result_minmax = dbscan.fit_predict(data_minmax)\n",
    "    result_maxabs = dbscan.fit_predict(data_maxabs)\n",
    "    result_standard = dbscan.fit_predict(data_standard)\n",
    "    result_robust = dbscan.fit_predict(data_robust)\n",
    "    result_normalize = dbscan.fit_predict(data_normalize)\n",
    "    result_log = dbscan.fit_predict(data_log)\n",
    "\n",
    "    class_count_minmax.append(len(np.unique(result_minmax)))\n",
    "    class_count_maxabs.append(len(np.unique(result_maxabs)))\n",
    "    class_count_standard.append(len(np.unique(result_standard)))\n",
    "    class_count_robust.append(len(np.unique(result_robust)))\n",
    "    class_count_normalize.append(len(np.unique(result_normalize)))\n",
    "    class_count_log.append(len(np.unique(result_log)))\n",
    "\n",
    "class_count_minmax = pd.DataFrame(class_count_minmax)\n",
    "class_count_minmax = class_count_minmax[class_count_minmax[0]==3]\n",
    "class_count_maxabs = pd.DataFrame(class_count_maxabs)\n",
    "class_count_maxabs = class_count_maxabs[class_count_maxabs[0]==3]\n",
    "class_count_standard = pd.DataFrame(class_count_standard)\n",
    "class_count_standard = class_count_standard[class_count_standard[0]==3]\n",
    "class_count_robust = pd.DataFrame(class_count_robust)\n",
    "class_count_robust = class_count_robust[class_count_robust[0]==3]\n",
    "class_count_normalize = pd.DataFrame(class_count_normalize)\n",
    "class_count_normalize = class_count_normalize[class_count_normalize[0]==3]\n",
    "class_count_log = pd.DataFrame(class_count_log)\n",
    "class_count_log = class_count_log[class_count_log[0]==3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_minmax, score_maxabs, score_standard, score_robust, score_normalize, score_log = [], [], [], [], [], []\n",
    "\n",
    "def minpts_score(arr, data, score_list):\n",
    "    for i in arr.index:\n",
    "        dbscan = DBSCAN(min_samples=i)\n",
    "        result = dbscan.fit_predict(data)\n",
    "        score_list.append(normalized_mutual_info_score(iris.target[0], result))\n",
    "\n",
    "minpts_score(class_count_minmax, data_minmax, score_minmax)\n",
    "minpts_score(class_count_maxabs, data_maxabs, score_maxabs)\n",
    "minpts_score(class_count_standard, data_standard, score_standard)\n",
    "minpts_score(class_count_robust, data_robust, score_robust)\n",
    "minpts_score(class_count_normalize, data_normalize, score_normalize)\n",
    "minpts_score(class_count_log, data_log, score_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "minmax score: 0\nmaxabs score: 0\nstandard score: 0.6496352668681216\nrobust score: 0.6716066484925268\nnormalize score: 0\nlog score: 0\n"
     ]
    }
   ],
   "source": [
    "def not_value(string, value):\n",
    "    try:\n",
    "        print(string, max(value))\n",
    "    except ValueError:\n",
    "        print(string, 0)\n",
    "\n",
    "not_value('minmax score:', score_minmax)\n",
    "not_value('maxabs score:', score_maxabs)\n",
    "not_value('standard score:', score_standard)\n",
    "not_value('robust score:', score_robust)\n",
    "not_value('normalize score:', score_normalize)\n",
    "not_value('log score:', score_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply TSNE\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "model = TSNE(n_components=2, random_state=0)\n",
    "\n",
    "data_minmax = MinMaxScaler().fit_transform(iris.data)\n",
    "data_maxabs = MaxAbsScaler().fit_transform(iris.data)\n",
    "data_standard = StandardScaler().fit_transform(iris.data)\n",
    "data_robust = RobustScaler().fit_transform(iris.data)\n",
    "data_normalize = Normalizer().fit_transform(iris.data)\n",
    "data_log = np.log1p(iris.data)\n",
    "\n",
    "data_minmax = model.fit_transform(data_minmax)\n",
    "data_maxabs = model.fit_transform(data_maxabs)\n",
    "data_standard = model.fit_transform(data_standard)\n",
    "data_robust = model.fit_transform(data_robust)\n",
    "data_normalize = model.fit_transform(data_normalize)\n",
    "data_log = model.fit_transform(data_log)\n",
    "\n",
    "class_count_minmax, class_count_maxabs, class_count_standard, class_count_robust, class_count_normalize, class_count_log = [],[],[],[],[],[]\n",
    "\n",
    "for i in range(1,100,1):\n",
    "    dbscan = DBSCAN(min_samples=i)\n",
    "    result_minmax = dbscan.fit_predict(data_minmax)\n",
    "    result_maxabs = dbscan.fit_predict(data_maxabs)\n",
    "    result_standard = dbscan.fit_predict(data_standard)\n",
    "    result_robust = dbscan.fit_predict(data_robust)\n",
    "    result_normalize = dbscan.fit_predict(data_normalize)\n",
    "    result_log = dbscan.fit_predict(data_log)\n",
    "\n",
    "    class_count_minmax.append(len(np.unique(result_minmax)))\n",
    "    class_count_maxabs.append(len(np.unique(result_maxabs)))\n",
    "    class_count_standard.append(len(np.unique(result_standard)))\n",
    "    class_count_robust.append(len(np.unique(result_robust)))\n",
    "    class_count_normalize.append(len(np.unique(result_normalize)))\n",
    "    class_count_log.append(len(np.unique(result_log)))\n",
    "\n",
    "class_count_minmax = pd.DataFrame(class_count_minmax)\n",
    "class_count_minmax = class_count_minmax[class_count_minmax[0]==3]\n",
    "class_count_maxabs = pd.DataFrame(class_count_maxabs)\n",
    "class_count_maxabs = class_count_maxabs[class_count_maxabs[0]==3]\n",
    "class_count_standard = pd.DataFrame(class_count_standard)\n",
    "class_count_standard = class_count_standard[class_count_standard[0]==3]\n",
    "class_count_robust = pd.DataFrame(class_count_robust)\n",
    "class_count_robust = class_count_robust[class_count_robust[0]==3]\n",
    "class_count_normalize = pd.DataFrame(class_count_normalize)\n",
    "class_count_normalize = class_count_normalize[class_count_normalize[0]==3]\n",
    "class_count_log = pd.DataFrame(class_count_log)\n",
    "class_count_log = class_count_log[class_count_log[0]==3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_minmax, score_maxabs, score_standard, score_robust, score_normalize, score_log = [], [], [], [], [], []\n",
    "\n",
    "def minpts_score(arr, data, score_list):\n",
    "    for i in arr.index:\n",
    "        dbscan = DBSCAN(min_samples=i)\n",
    "        result = dbscan.fit_predict(data)\n",
    "        score_list.append(normalized_mutual_info_score(iris.target[0], result))\n",
    "\n",
    "minpts_score(class_count_minmax, data_minmax, score_minmax)\n",
    "minpts_score(class_count_maxabs, data_maxabs, score_maxabs)\n",
    "minpts_score(class_count_standard, data_standard, score_standard)\n",
    "minpts_score(class_count_robust, data_robust, score_robust)\n",
    "minpts_score(class_count_normalize, data_normalize, score_normalize)\n",
    "minpts_score(class_count_log, data_log, score_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "minmax score: 0.27251776618860857\nmaxabs score: 0\nstandard score: 0.2124641071366041\nrobust score: 0\nnormalize score: 0.28079853190582854\nlog score: 0\n"
     ]
    }
   ],
   "source": [
    "def not_value(string, value):\n",
    "    try:\n",
    "        print(string, max(value))\n",
    "    except ValueError:\n",
    "        print(string, 0)\n",
    "\n",
    "not_value('minmax score:', score_minmax)\n",
    "not_value('maxabs score:', score_maxabs)\n",
    "not_value('standard score:', score_standard)\n",
    "not_value('robust score:', score_robust)\n",
    "not_value('normalize score:', score_normalize)\n",
    "not_value('log score:', score_log)"
   ]
  }
 ]
}