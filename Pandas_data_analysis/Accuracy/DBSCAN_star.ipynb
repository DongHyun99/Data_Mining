{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "ee6caf572b02fbd90c9d9a82b73a7abd542127d236c55761b702f851ee4a295f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 2. DBSCAN으로 학습하기 (star)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.datasets import load_wine\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-2-5d83e6dcfeb2>:3: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n  iris.data = pd.read_csv(\"dataset/Stars.csv\")\n<ipython-input-2-5d83e6dcfeb2>:4: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n  iris.target = pd.DataFrame(iris.data.Type)\n"
     ]
    }
   ],
   "source": [
    "# data load\n",
    "iris=pd.DataFrame(['data','target'])\n",
    "iris.data = pd.read_csv(\"dataset/Stars.csv\")\n",
    "iris.target = pd.DataFrame(iris.data.Type)\n",
    "iris.data = iris.data.drop(['Color', 'Spectral_Class', 'Type'], axis=1)\n",
    "iris.data.columns=[0,1,2,3]\n",
    "iris.target.columns = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\neps의 값은 항상 positive 해야 하므로 for문에 넣을 수 없다.\\n따라서 계획 했던대로 eps의 값을 증가 시켜보며 학습하는 것은 힘들고\\nminpts의 값을 증가시키면서 학습해보겠다.\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# Specify the eps value\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "'''\n",
    "eps의 값은 항상 positive 해야 하므로 for문에 넣을 수 없다.\n",
    "따라서 계획 했던대로 eps의 값을 증가 시켜보며 학습하는 것은 힘들고\n",
    "minpts의 값을 증가시키면서 학습해보겠다.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Empty DataFrame\nColumns: [0]\nIndex: []\n"
     ]
    }
   ],
   "source": [
    "# class count graph with minpts change\n",
    "# min_samples는 1~100까지 돌려본다.\n",
    "\n",
    "class_count=[0,]\n",
    "\n",
    "for i in range(1,100,1):\n",
    "    dbscan = DBSCAN(min_samples=i)\n",
    "    result = dbscan.fit_predict(iris.data)\n",
    "    class_count.append(len(np.unique(result)))\n",
    "\n",
    "class_count = pd.DataFrame(class_count)\n",
    "class_count = class_count[class_count[0]==6]\n",
    "print(class_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normal_iris_dbscan accuracy\n",
    "# class의 개수가 데이터의 개수와 맞는 것의 accuracy를 측정해본다.\n",
    "\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "\n",
    "for i in class_count.index:\n",
    "    dbscan = DBSCAN(min_samples=i)\n",
    "    result = dbscan.fit_predict(iris.data)\n",
    "    print('minpts=',i,\":\",normalized_mutual_info_score(iris.target[0], result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaler Preprocessed DBSCAN Accuracy\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "data_minmax = MinMaxScaler().fit_transform(iris.data)\n",
    "data_maxabs = MaxAbsScaler().fit_transform(iris.data)\n",
    "data_standard = StandardScaler().fit_transform(iris.data)\n",
    "data_robust = RobustScaler().fit_transform(iris.data)\n",
    "data_normalize = Normalizer().fit_transform(iris.data)\n",
    "data_log = np.log1p(iris.data)\n",
    "\n",
    "class_count_minmax, class_count_maxabs, class_count_standard, class_count_robust, class_count_normalize, class_count_log = [],[],[],[],[],[]\n",
    "\n",
    "for i in range(1,100,1):\n",
    "    dbscan = DBSCAN(min_samples=i)\n",
    "    result_minmax = dbscan.fit_predict(data_minmax)\n",
    "    result_maxabs = dbscan.fit_predict(data_maxabs)\n",
    "    result_standard = dbscan.fit_predict(data_standard)\n",
    "    result_robust = dbscan.fit_predict(data_robust)\n",
    "    result_normalize = dbscan.fit_predict(data_normalize)\n",
    "\n",
    "    class_count_minmax.append(len(np.unique(result_minmax)))\n",
    "    class_count_maxabs.append(len(np.unique(result_maxabs)))\n",
    "    class_count_standard.append(len(np.unique(result_standard)))\n",
    "    class_count_robust.append(len(np.unique(result_robust)))\n",
    "    class_count_normalize.append(len(np.unique(result_normalize)))\n",
    "\n",
    "class_count_minmax = pd.DataFrame(class_count_minmax)\n",
    "class_count_minmax = class_count_minmax[class_count_minmax[0]==6]\n",
    "class_count_maxabs = pd.DataFrame(class_count_maxabs)\n",
    "class_count_maxabs = class_count_maxabs[class_count_maxabs[0]==6]\n",
    "class_count_standard = pd.DataFrame(class_count_standard)\n",
    "class_count_standard = class_count_standard[class_count_standard[0]==6]\n",
    "class_count_robust = pd.DataFrame(class_count_robust)\n",
    "class_count_robust = class_count_robust[class_count_robust[0]==6]\n",
    "class_count_normalize = pd.DataFrame(class_count_normalize)\n",
    "class_count_normalize = class_count_normalize[class_count_normalize[0]==6]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_minmax, score_maxabs, score_standard, score_robust, score_normalize, score_log = [], [], [], [], [], []\n",
    "\n",
    "def minpts_score(arr, data, score_list):\n",
    "    for i in arr.index:\n",
    "        dbscan = DBSCAN(min_samples=i)\n",
    "        result = dbscan.fit_predict(data)\n",
    "        score_list.append(normalized_mutual_info_score(iris.target[0], result))\n",
    "\n",
    "minpts_score(class_count_minmax, data_minmax, score_minmax)\n",
    "minpts_score(class_count_maxabs, data_maxabs, score_maxabs)\n",
    "minpts_score(class_count_standard, data_standard, score_standard)\n",
    "minpts_score(class_count_robust, data_robust, score_robust)\n",
    "minpts_score(class_count_normalize, data_normalize, score_normalize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "minmax score: 0\nmaxabs score: 0\nstandard score: 0.5261509730414484\nrobust score: 0\nnormalize score: 0\n"
     ]
    }
   ],
   "source": [
    "def not_value(string, value):\n",
    "    try:\n",
    "        print(string, max(value))\n",
    "    except ValueError:\n",
    "        print(string, 0)\n",
    "\n",
    "not_value('minmax score:', score_minmax)\n",
    "not_value('maxabs score:', score_maxabs)\n",
    "not_value('standard score:', score_standard)\n",
    "not_value('robust score:', score_robust)\n",
    "not_value('normalize score:', score_normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "model = PCA(n_components=2)\n",
    "\n",
    "data_minmax = MinMaxScaler().fit_transform(iris.data)\n",
    "data_maxabs = MaxAbsScaler().fit_transform(iris.data)\n",
    "data_standard = StandardScaler().fit_transform(iris.data)\n",
    "data_robust = RobustScaler().fit_transform(iris.data)\n",
    "data_normalize = Normalizer().fit_transform(iris.data)\n",
    "\n",
    "data_minmax = model.fit_transform(data_minmax)\n",
    "data_maxabs = model.fit_transform(data_maxabs)\n",
    "data_standard = model.fit_transform(data_standard)\n",
    "data_robust = model.fit_transform(data_robust)\n",
    "data_normalize = model.fit_transform(data_normalize)\n",
    "\n",
    "class_count_minmax, class_count_maxabs, class_count_standard, class_count_robust, class_count_normalize, class_count_log = [],[],[],[],[],[]\n",
    "\n",
    "for i in range(1,100,1):\n",
    "    dbscan = DBSCAN(min_samples=i)\n",
    "    result_minmax = dbscan.fit_predict(data_minmax)\n",
    "    result_maxabs = dbscan.fit_predict(data_maxabs)\n",
    "    result_standard = dbscan.fit_predict(data_standard)\n",
    "    result_robust = dbscan.fit_predict(data_robust)\n",
    "    result_normalize = dbscan.fit_predict(data_normalize)\n",
    "\n",
    "    class_count_minmax.append(len(np.unique(result_minmax)))\n",
    "    class_count_maxabs.append(len(np.unique(result_maxabs)))\n",
    "    class_count_standard.append(len(np.unique(result_standard)))\n",
    "    class_count_robust.append(len(np.unique(result_robust)))\n",
    "    class_count_normalize.append(len(np.unique(result_normalize)))\n",
    "\n",
    "class_count_minmax = pd.DataFrame(class_count_minmax)\n",
    "class_count_minmax = class_count_minmax[class_count_minmax[0]==6]\n",
    "class_count_maxabs = pd.DataFrame(class_count_maxabs)\n",
    "class_count_maxabs = class_count_maxabs[class_count_maxabs[0]==6]\n",
    "class_count_standard = pd.DataFrame(class_count_standard)\n",
    "class_count_standard = class_count_standard[class_count_standard[0]==6]\n",
    "class_count_robust = pd.DataFrame(class_count_robust)\n",
    "class_count_robust = class_count_robust[class_count_robust[0]==6]\n",
    "class_count_normalize = pd.DataFrame(class_count_normalize)\n",
    "class_count_normalize = class_count_normalize[class_count_normalize[0]==6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_minmax, score_maxabs, score_standard, score_robust, score_normalize, score_log = [], [], [], [], [], []\n",
    "\n",
    "def minpts_score(arr, data, score_list):\n",
    "    for i in arr.index:\n",
    "        dbscan = DBSCAN(min_samples=i)\n",
    "        result = dbscan.fit_predict(data)\n",
    "        score_list.append(normalized_mutual_info_score(iris.target[0], result))\n",
    "\n",
    "minpts_score(class_count_minmax, data_minmax, score_minmax)\n",
    "minpts_score(class_count_maxabs, data_maxabs, score_maxabs)\n",
    "minpts_score(class_count_standard, data_standard, score_standard)\n",
    "minpts_score(class_count_robust, data_robust, score_robust)\n",
    "minpts_score(class_count_normalize, data_normalize, score_normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "minmax score: 0\nmaxabs score: 0\nstandard score: 0.32127898808148164\nrobust score: 0\nnormalize score: 0\n"
     ]
    }
   ],
   "source": [
    "def not_value(string, value):\n",
    "    try:\n",
    "        print(string, max(value))\n",
    "    except ValueError:\n",
    "        print(string, 0)\n",
    "\n",
    "not_value('minmax score:', score_minmax)\n",
    "not_value('maxabs score:', score_maxabs)\n",
    "not_value('standard score:', score_standard)\n",
    "not_value('robust score:', score_robust)\n",
    "not_value('normalize score:', score_normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply TSNE\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "model = TSNE(n_components=2, random_state=0)\n",
    "\n",
    "data_minmax = MinMaxScaler().fit_transform(iris.data)\n",
    "data_maxabs = MaxAbsScaler().fit_transform(iris.data)\n",
    "data_standard = StandardScaler().fit_transform(iris.data)\n",
    "data_robust = RobustScaler().fit_transform(iris.data)\n",
    "data_normalize = Normalizer().fit_transform(iris.data)\n",
    "\n",
    "data_minmax = model.fit_transform(data_minmax)\n",
    "data_maxabs = model.fit_transform(data_maxabs)\n",
    "data_standard = model.fit_transform(data_standard)\n",
    "data_robust = model.fit_transform(data_robust)\n",
    "data_normalize = model.fit_transform(data_normalize)\n",
    "\n",
    "class_count_minmax, class_count_maxabs, class_count_standard, class_count_robust, class_count_normalize, class_count_log = [],[],[],[],[],[]\n",
    "\n",
    "for i in range(1,100,1):\n",
    "    dbscan = DBSCAN(min_samples=i)\n",
    "    result_minmax = dbscan.fit_predict(data_minmax)\n",
    "    result_maxabs = dbscan.fit_predict(data_maxabs)\n",
    "    result_standard = dbscan.fit_predict(data_standard)\n",
    "    result_robust = dbscan.fit_predict(data_robust)\n",
    "    result_normalize = dbscan.fit_predict(data_normalize)\n",
    "\n",
    "    class_count_minmax.append(len(np.unique(result_minmax)))\n",
    "    class_count_maxabs.append(len(np.unique(result_maxabs)))\n",
    "    class_count_standard.append(len(np.unique(result_standard)))\n",
    "    class_count_robust.append(len(np.unique(result_robust)))\n",
    "    class_count_normalize.append(len(np.unique(result_normalize)))\n",
    "\n",
    "class_count_minmax = pd.DataFrame(class_count_minmax)\n",
    "class_count_minmax = class_count_minmax[class_count_minmax[0]==3]\n",
    "class_count_maxabs = pd.DataFrame(class_count_maxabs)\n",
    "class_count_maxabs = class_count_maxabs[class_count_maxabs[0]==3]\n",
    "class_count_standard = pd.DataFrame(class_count_standard)\n",
    "class_count_standard = class_count_standard[class_count_standard[0]==3]\n",
    "class_count_robust = pd.DataFrame(class_count_robust)\n",
    "class_count_robust = class_count_robust[class_count_robust[0]==3]\n",
    "class_count_normalize = pd.DataFrame(class_count_normalize)\n",
    "class_count_normalize = class_count_normalize[class_count_normalize[0]==3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_minmax, score_maxabs, score_standard, score_robust, score_normalize, score_log = [], [], [], [], [], []\n",
    "\n",
    "def minpts_score(arr, data, score_list):\n",
    "    for i in arr.index:\n",
    "        dbscan = DBSCAN(min_samples=i)\n",
    "        result = dbscan.fit_predict(data)\n",
    "        score_list.append(normalized_mutual_info_score(iris.target[0], result))\n",
    "\n",
    "minpts_score(class_count_minmax, data_minmax, score_minmax)\n",
    "minpts_score(class_count_maxabs, data_maxabs, score_maxabs)\n",
    "minpts_score(class_count_standard, data_standard, score_standard)\n",
    "minpts_score(class_count_robust, data_robust, score_robust)\n",
    "minpts_score(class_count_normalize, data_normalize, score_normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "minmax score: 0.20099914628911666\nmaxabs score: 0.20636085448441382\nstandard score: 0\nrobust score: 0.3238048201925838\nnormalize score: 0.2057305838852248\n"
     ]
    }
   ],
   "source": [
    "def not_value(string, value):\n",
    "    try:\n",
    "        print(string, max(value))\n",
    "    except ValueError:\n",
    "        print(string, 0)\n",
    "\n",
    "not_value('minmax score:', score_minmax)\n",
    "not_value('maxabs score:', score_maxabs)\n",
    "not_value('standard score:', score_standard)\n",
    "not_value('robust score:', score_robust)\n",
    "not_value('normalize score:', score_normalize)"
   ]
  }
 ]
}