{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "ee6caf572b02fbd90c9d9a82b73a7abd542127d236c55761b702f851ee4a295f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 1. K-means로 학습하기"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data load\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "iris.data = pd.DataFrame(iris.data)\n",
    "iris.target = pd.DataFrame(iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10번 반복하는 pipeline 함수 제작\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def pipe(model: Pipeline, train, target):\n",
    "    result = []\n",
    "    for i in range(0,10,1):\n",
    "        model.fit(train, target)\n",
    "        result.append(model.predict(train))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.7581756800057784\n"
     ]
    }
   ],
   "source": [
    "# normal_iris_kmeans accuracy\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "pipeline = Pipeline([('Kmeans', KMeans(n_clusters=3, init='random'))]) # normal predict 생성\n",
    "result_norm = pipe(pipeline, iris.data, iris.target)\n",
    "\n",
    "\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "from sklearn.metrics.cluster import adjusted_rand_score # Adjusted rand score는 음수가 나올수 있으므로 백분율 계산을 위해서 Normalized mutual information을 사용했다. \n",
    "\n",
    "score = []\n",
    "for i in result_norm:\n",
    "    score.append(normalized_mutual_info_score(iris.target[0], i))\n",
    "\n",
    "print(np.mean(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "minmax_pca: 0.7419116631817836\nmaxabs_pca: 0.8641855068202222\nstandard_pca: 0.6548417952241196\nrobust_pca: 0.6162206156967885\nnormalize_pca: 0.8996935451597474\nlog_scaled_pca: 0.8464828103876364\n"
     ]
    }
   ],
   "source": [
    "# PCA _iris_kemans accuracy\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# 각 정규화 된 값들을 PCA로 차원축소 시키고 값이 어떻게 변화하는지 살펴본다.\n",
    "# 일단 컴포넌트는 2로 하고 1~3중 어느값이 가장 좋은 결과를 가지는지 학습시켜본다.\n",
    "pipeline_minmax = Pipeline([('MinMax', MinMaxScaler()),('PCA', PCA(n_components=2)),('Kmeans', KMeans(n_clusters=3, init='random'))])\n",
    "pipeline_maxabs = Pipeline([('MaxAbs', MaxAbsScaler()),('PCA', PCA(n_components=2)),('Kmeans', KMeans(n_clusters=3, init='random'))])\n",
    "pipeline_standard = Pipeline([('Standard', StandardScaler()),('PCA', PCA(n_components=2)),('Kmeans', KMeans(n_clusters=3, init='random'))])\n",
    "pipeline_robust = Pipeline([('Robust', RobustScaler()),('PCA', PCA(n_components=2)),('Kmeans', KMeans(n_clusters=3, init='random'))])\n",
    "pipeline_normalize = Pipeline([('Normalize', Normalizer()),('PCA', PCA(n_components=2)),('Kmeans', KMeans(n_clusters=3, init='random'))])\n",
    "pipeline_log = Pipeline([('PCA', PCA(n_components=2)),('Kmeans', KMeans(n_clusters=3, init='random'))])\n",
    "\n",
    "result_minmax = pipe(pipeline_minmax, iris.data, iris.target)\n",
    "result_maxabs = pipe(pipeline_maxabs, iris.data, iris.target)\n",
    "result_standard = pipe(pipeline_standard, iris.data, iris.target)\n",
    "result_robust = pipe(pipeline_robust, iris.data, iris.target)\n",
    "result_normalize = pipe(pipeline_normalize, iris.data, iris.target)\n",
    "result_log = pipe(pipeline_log, np.log1p(iris.data), iris.target)\n",
    "\n",
    "score_minmax, score_maxabs, score_standard, score_robust, score_normalize, score_log = [], [], [], [], [], []\n",
    "for i in range(0,10,1):\n",
    "    score_minmax.append(normalized_mutual_info_score(iris.target[0], result_minmax[i]))\n",
    "    score_maxabs.append(normalized_mutual_info_score(iris.target[0], result_maxabs[i]))\n",
    "    score_standard.append(normalized_mutual_info_score(iris.target[0], result_standard[i]))\n",
    "    score_robust.append(normalized_mutual_info_score(iris.target[0], result_robust[i]))\n",
    "    score_normalize.append(normalized_mutual_info_score(iris.target[0], result_normalize[i]))\n",
    "    score_log.append(normalized_mutual_info_score(iris.target[0], result_log[i]))\n",
    "\n",
    "print('minmax_pca:',np.mean(score_minmax))\n",
    "print('maxabs_pca:',np.mean(score_maxabs))\n",
    "print('standard_pca:',np.mean(score_standard))\n",
    "print('robust_pca:',np.mean(score_robust))\n",
    "print('normalize_pca:',np.mean(score_normalize))\n",
    "print('log_scaled_pca:',np.mean(score_log))\n",
    "\n",
    "# normalizer의 accuracy가 가장 높으므로 normalizer로 전처리된 값을 이용해 PCA의 컴포넌트를 1~3까지 넣어본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "component=1: 0.9305506621576433\ncomponent=2: 0.8996935451597474\ncomponent=3: 0.8996935451597474\n"
     ]
    }
   ],
   "source": [
    "# PCA _iris_kemans accuracy (change_component)\n",
    "\n",
    "pipeline1 = Pipeline([('Normalize', Normalizer()),('PCA', PCA(n_components=1)),('Kmeans', KMeans(n_clusters=3, init='random'))])\n",
    "pipeline2 = Pipeline([('Normalize', Normalizer()),('PCA', PCA(n_components=2)),('Kmeans', KMeans(n_clusters=3, init='random'))])\n",
    "pipeline3 = Pipeline([('Normalize', Normalizer()),('PCA', PCA(n_components=3)),('Kmeans', KMeans(n_clusters=3, init='random'))])\n",
    "\n",
    "result1 = pipe(pipeline1, iris.data, iris.target)\n",
    "result2 = pipe(pipeline2, iris.data, iris.target)\n",
    "result3 = pipe(pipeline3, iris.data, iris.target)\n",
    "\n",
    "score1, score2, score3 = [], [], []\n",
    "\n",
    "for i in range(0,10,1):\n",
    "    score1.append(normalized_mutual_info_score(iris.target[0], result1[i]))\n",
    "    score2.append(normalized_mutual_info_score(iris.target[0], result2[i]))\n",
    "    score3.append(normalized_mutual_info_score(iris.target[0], result3[i]))\n",
    "\n",
    "print('component=1:',np.mean(score1))\n",
    "print('component=2:',np.mean(score2))\n",
    "print('component=3:',np.mean(score3))\n",
    "\n",
    "# component는 1개일 때 가장 좋은 결과를 보였다. 다만 dataset 마다 결과는 다를 것으로보인다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "All intermediate steps should be transformers and implement fit and transform or be the string 'passthrough' 'TSNE(random_state=0)' (type <class 'sklearn.manifold._t_sne.TSNE'>) doesn't",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-153-49a6d0013de1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmanifold\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpipeline_minmax\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'MinMax'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m't-SNE'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Kmeans'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'random'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mpipeline_maxabs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'MaxAbs'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMaxAbsScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m't-SNE'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Kmeans'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'random'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpipeline_standard\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Standard'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m't-SNE'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Kmeans'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'random'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, steps, memory, verbose)\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_steps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_validate_steps\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    166\u001b[0m             if (not (hasattr(t, \"fit\") or hasattr(t, \"fit_transform\")) or not\n\u001b[0;32m    167\u001b[0m                     hasattr(t, \"transform\")):\n\u001b[1;32m--> 168\u001b[1;33m                 raise TypeError(\"All intermediate steps should be \"\n\u001b[0m\u001b[0;32m    169\u001b[0m                                 \u001b[1;34m\"transformers and implement fit and transform \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m                                 \u001b[1;34m\"or be the string 'passthrough' \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: All intermediate steps should be transformers and implement fit and transform or be the string 'passthrough' 'TSNE(random_state=0)' (type <class 'sklearn.manifold._t_sne.TSNE'>) doesn't"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "pipeline_minmax = Pipeline([('MinMax', MinMaxScaler()),('t-SNE', TSNE(random_state=0)),('Kmeans', KMeans(n_clusters=3, init='random'))])\n",
    "pipeline_maxabs = Pipeline([('MaxAbs', MaxAbsScaler()),('t-SNE', TSNE(random_state=0)),('Kmeans', KMeans(n_clusters=3, init='random'))])\n",
    "pipeline_standard = Pipeline([('Standard', StandardScaler()),('t-SNE', TSNE(random_state=0)),('Kmeans', KMeans(n_clusters=3, init='random'))])\n",
    "pipeline_robust = Pipeline([('Robust', RobustScaler()),('t-SNE', TSNE(random_state=0)),('Kmeans', KMeans(n_clusters=3, init='random'))])\n",
    "pipeline_normalize = Pipeline([('Normalize', Normalizer()),('t-SNE', TSNE(random_state=0)),('Kmeans', KMeans(n_clusters=3, init='random'))])\n",
    "pipeline_log = Pipeline([('t-SNE', TSNE(random_state=0)),('Kmeans', KMeans(n_clusters=3, init='random'))])\n",
    "\n",
    "result_minmax = pipe(pipeline_minmax, iris.data, iris.target)\n",
    "result_maxabs = pipe(pipeline_maxabs, iris.data, iris.target)\n",
    "result_standard = pipe(pipeline_standard, iris.data, iris.target)\n",
    "result_robust = pipe(pipeline_robust, iris.data, iris.target)\n",
    "result_normalize = pipe(pipeline_normalize, iris.data, iris.target)\n",
    "result_log = pipe(pipeline_log, np.log1p(iris.data), iris.target)\n",
    "\n",
    "score_minmax, score_maxabs, score_standard, score_robust, score_normalize, score_log = [], [], [], [], [], []\n",
    "for i in range(0,10,1):\n",
    "    score_minmax.append(normalized_mutual_info_score(iris.target[0], result_minmax[i]))\n",
    "    score_maxabs.append(normalized_mutual_info_score(iris.target[0], result_maxabs[i]))\n",
    "    score_standard.append(normalized_mutual_info_score(iris.target[0], result_standard[i]))\n",
    "    score_robust.append(normalized_mutual_info_score(iris.target[0], result_robust[i]))\n",
    "    score_normalize.append(normalized_mutual_info_score(iris.target[0], result_normalize[i]))\n",
    "    score_log.append(normalized_mutual_info_score(iris.target[0], result_log[i]))\n",
    "\n",
    "print('minmax_TSNE:',np.mean(score_minmax))\n",
    "print('maxabs_TSNE:',np.mean(score_maxabs))\n",
    "print('standard_TSNE:',np.mean(score_standard))\n",
    "print('robust_TSNE:',np.mean(score_robust))\n",
    "print('normalize_TSNE:',np.mean(score_normalize))\n",
    "print('log_scaled_TSNE:',np.mean(score_log))"
   ]
  }
 ]
}